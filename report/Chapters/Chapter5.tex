% Chapter 5

\chapter{Adversarial Networks Training} % Main chapter title
\label{Chapter5} % For referencing the chapter elsewhere, use \ref{Chapter1} 

Now that we built our dataset, we have the data we need to proceed with the adversarial training of our pipeline.

It is once again important to place this chapter in the context of our data pipeline, as we introduced it in section~\ref{sec:datapipeline} and as we showed it in figure~\ref{fig:pipeline}. The dataset we just created is composed of policies (which are just tables or 2D matrices).  More specifically we have 3,827 such policies, corresponding to all the 3,827 different valid map configurations of our environment, \code{RandomisedFrozenLake}. We split the dataset in a training subset (80\%) of the total number of policies, and the test subset (the remaining 20\%).

In section~\ref{sec:gan} we introduced Generative Adversarial Networks, detailing their vanilla architecture, as they were introduced by \citeauthor{goodfellow2014generative}. We now build off of this architecture to build a GAN architecture for policy generation.

In this section we present such architecture, explain the design decisions that were involved in configuring it, and describe the process of training it in an adversarial manner. We also provide a detailed breakdown of the generator and the discriminator's deep neural networks. We also make our Keras implementation of our Policy GAN publicly available.

The input to our Policy GAN will only be the training subset of the policies we trained with Q-learning. In this way, we hope to be able to model, through our GAN, a distribution on these policies such that we can capture knowledge that we can transfer to other unseen tasks.
These unseen tasks are exactly the ones corresponding to the map configurations of the test subset, and we therefore do not include those test policies as our input to our GAN.

%----------------------------------------------------------------------------------------

\section{Policy GAN}
In figure~\ref{fig:PolicyGAN} we show the architecture of our Policy GAN.
We already described most of the details involved when training GANs in section~\ref{sec:gan}. 

Training a GAN is an iterative process that runs for a set amount of epochs (an epoch is one full training cycle on the training set).
At each iteration, the mini-batch inputs to the discriminator \code{D} are taken from both the real data sampled from the training data (in our case policies from the training set that we created in chapter~\ref{Chapter4}), and the samples generated by the generator.
Given \code{D}'s prediction, we then apply a gradient-based optimisation method to update both \code{D}'s and \code{G}'s parameters.

Building neural net architecture that work well in practice generally involves finding and tuning many parameters and making several design decisions that make the issue not trivial. In the following subsections, we introduce such decisions and hyperparameters choice with the aim of allowing reproducibility of our results.

We report here the architectures of final optimal deep neural networks for the Generator model and the Discriminator model. Figure~\ref{fig:Generator} and figure~\ref{fig:Discriminator} show a visualisation for the two networks respectively.\\\\
\textbf{Generator}:
\begin{itemize}[noitemsep]
	\item Input: 100 dim
	\item Hidden Layer 1: 256 dim
	\begin{itemize}[noitemsep,topsep=0pt]
		\item Activation: LeakyReLu(alpha = 0.2)
		\item BatchNormalisation(momentum = 0.8)
	\end{itemize}
	\item Hidden Layer 2: 512 dim
	\begin{itemize}[noitemsep,topsep=0pt]
		\item Activation: LeakyReLu(alpha = 0.2)
		\item BatchNormalisation(momentum = 0.8)
	\end{itemize}
		\item Hidden Layer 3: 1024 dim
	\begin{itemize}[noitemsep,topsep=0pt]
		\item Activation: LeakyReLu(alpha = 0.2)
		\item BatchNormalisation(momentum = 0.8)
	\end{itemize}
	\item Output Layer: 64 dim
	\begin{itemize}[noitemsep,topsep=0pt]
		\item Activation: tanh
	\end{itemize}
\end{itemize}
\phantom\\
\textbf{Discriminator}:
\begin{itemize}[noitemsep]
	\item Input: 64 dim
	\item Hidden Layer 1: 512 dim
	\begin{itemize}[noitemsep,topsep=0pt]
		\item Activation: LeakyReLu(alpha = 0.2)
	\end{itemize}
	\item Hidden Layer 2: 256 dim
	\begin{itemize}[noitemsep,topsep=0pt]
		\item Activation: LeakyReLu(alpha = 0.2)
	\end{itemize}
	\item Output Layer: 1 dim
	\begin{itemize}[noitemsep,topsep=0pt]
		\item Activation: sigmoid
	\end{itemize}
\end{itemize}

As we can notice, the Generator network outputs a vector of size dimension 64, which we can then reshape to a 16x4 matrix. This is our 'fake', generated Q-table. Its input is just a the latent space vector $z$ of size 100 whose values we can set randomly. We can interpret $z$ as a noise vector which deterministically influences the Q-table that we generate. While we can vary $z$ and obtain different Q-tables, these generated Q-tables should still fall within the initial distributions of Q-tables we have in our dataset, provided that the Generator was trained properly.

The Discriminator network, in turn, takes in an input a vector of size 64 (which is a reshaped 16x4 Q-table), and outputs a single scalar in $[0,1]$ representing the predicted probability that the inputted Q-table is a generated policy or one sampled from the dataset.

\begin{figure}
\centering
\includegraphics[width=15cm]{Figures/Generator}
\caption{Architecture of the Generator network}
\label{fig:Generator}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=10cm]{Figures/Discriminator}
\caption{Architecture of the Discriminator network}
\label{fig:Discriminator}
\end{figure}

%TODO Graph losses Generator Discriminator

\begin{figure}
\centering
\includegraphics[width=15cm]{Figures/PolicyGAN}
\caption{Our Policy GAN architecture}
\label{fig:PolicyGAN}
\end{figure}

\subsection{Gradient descent optimisation}
Learning rates and gradient descent optimisations algorithms have historically been among the trickiest hyperparameters to set, as they drastically influence the final results, perhaps more than all other hyperparameters.

Gradient descent \citep{lemarechal2012cauchy}, a seminal approach to optimisation that dates back to Cauchy a few centuries back, still proves relatively successful in many machine learning applications.
Since then, a lot of work has been put in devising algorithms that have adaptive learning rates and that lead to faster convergence. In our design, we specifically explored two such adaptive learning rates algorithms: RMSProp \citep{hinton2012neural} and Adam \citep{DBLP:journals/corr/KingmaB14}

Both perform local optimisation with different techniques and metrics that are constructed from the history of previous interactions.

RMSProp (algorithm \ref{alg:RMSProp}) is a modification of the AdaGrad optimiser \citep{duchi2011adaptive}, that modifies the gradient accumulation into a moving average that is exponentially weighted.

Adam (algorithm \ref{alg:Adam}) is best described as a combination of RMSProp and Stochastic Gradient Descent (SGD) with momentum \citep{sutskever2013importance}. Momentum accelerates SGD by multiplying the learning rate by a parameter that increases as we go towards the right direction in the gradient update.

\begin{algorithm}[ht]
\begin{algorithmic}
   \STATE {\bfseries Require:} Global learning rate $\epsilon$, decay rate $\rho$
   \STATE {\bfseries Require:} Initial parameter $\theta$
   \STATE {\bfseries Require:} Small constant $\delta$, usually $10^{-6}$, used to stabilize division by small number
   
   \STATE Initialize accumulation variables $\boldsymbol{r=0}$
   \WHILE{stopping criterion not met}{}
   \STATE Sample a minibatch of $m$ examples from the training set $\{x^{(1)},...,x^{(m)}\}$ with corresponding targets $y^{i}$.
   \STATE Compute gradient: $\boldsymbol{g \leftarrow \frac{1}{m}\nabla_\theta \sum_i L(f(x^{(i)}; \theta), y^{(i)}).}$
   \STATE Accumulate squared gradient: \\ $\boldsymbol{r \leftarrow \rho r + (1 - \rho) g \odot g}$
   \STATE Compute parameter update: $\boldsymbol{\Delta\theta = -\frac{\epsilon}{\sqrt{\delta+r}} \odot g.}$ ($\frac{1}{\sqrt{\delta+r}}$ applied element-wise)
   \STATE Apply update: $\boldsymbol{\theta \leftarrow \theta + \Delta \theta}$
   \ENDWHILE
\end{algorithmic}
  \caption{RMSProp algorithm}
  \label{alg:RMSProp}
\end{algorithm}


\begin{algorithm}[ht]
\begin{algorithmic}
   \STATE {\bfseries Require:} Step size $\epsilon$ (Suggested default: 0.001)
   \STATE {\bfseries Require:} Exponential decay rates for moment estimates, $\rho_1$ and $\rho_2$ in $[0,1)$. (Suggested defaults: 0.9 and 0.999 respectively)
   \STATE {\bfseries Require:} Small constant $\delta$ used for numerical stabilisation (Suggested default: $10^{-8}$)
   \STATE {\bfseries Require:} Initial parameters $\theta$
   
   \STATE Initialise 1st and 2nd moment variables $\mathbf{s=0, r=0}$
   \STATE Initialise time step $t=0$
   \WHILE{stopping criterion not met}{}
   \STATE Sample a minibatch of $m$ examples from the training set $\{x^{(1)},...,x^{(m)}\}$ with corresponding targets $y^{i}$.
   \STATE Compute gradient: $\boldsymbol{g \leftarrow \frac{1}{m}\nabla_\theta \sum_i L(f(x^{(i)}; \theta), y^{(i)}).}$
   \STATE $t \leftarrow t + 1$
   \STATE Update biased first moment estimate: $\boldsymbol{s \leftarrow \rho_1 s + (1-\rho_1) g}$
   \STATE Update biased second moment estimate: $\boldsymbol{r \leftarrow \rho_2 r + (1-\rho_2) g \odot g}$
   \STATE Correct bias in first moment: $\boldsymbol{\hat{s} \leftarrow \frac{s}{1-\rho_1^t}}$
   \STATE Correct bias in second moment: $\boldsymbol{\hat{r} \leftarrow \frac{r}{1-\rho_2^t}}$
   \STATE Compute update: $\boldsymbol{\Delta \theta = -\epsilon \frac{\hat{s}}{\sqrt{\hat{r}} + \delta}}$
   \STATE Apply update: $\boldsymbol{\theta \leftarrow \theta + \Delta \theta}$
   \ENDWHILE
\end{algorithmic}
  \caption{Adam algorithm}
  \label{alg:Adam}
\end{algorithm}

Like in most applications involving deep learning, there is no definite way to see which approach would yield the best results without going through the whole training process.

We therefore empirically experiment with each of these approaches to see which choice is more suitable to our data distribution and yields the best results.

"Best", in our case, is seen in the context of both the Generator network and the Discriminator network. We look for an optimisation technique that leads to stable costs for both networks. In most deep learning problems, convergence of the cost function is limited to one single neural network. In adversarial architectures like GANs, or architectures with multiple deep neural networks like Variational Autoencoders (VAE), or Actor-Critic networks, convergence of the model is not a property that we can evaluate as trivially \citep{kingma2013auto, grondman2012survey}.

We look for an asymptotic behaviour that has both networks converge to an optimum, without having one network prevail over the other. For example, we could have a Discriminator that produces perfect predictions after a few epochs of training, but that is a meaningless result if the Generator is not able to produce results that can "fool" the discriminative model.

The overall cost of the architecture, in short, must take into account both components. The cost of the whole architecture has the following closed form:
\[\min_{G} \max_{D} V(G, D) = \mathbb{E}_{x\sim p_{data}(x)}[log D(x)] + \mathbb{E}_{z \sim p_{g}(z)}[log(1-D(G(z)))] \]

With both RMSProp and Adam, we obtain a much better convergence compared to vanilla SGD. While we do not notice a noticeable difference between these two results, having adaptive learning rates clearly helps convergence for both models.

After optimising our hyperparameters with grid-search, our final optimal optimisation algorithm is the following: \\
Adam, step size $\epsilon = 0.0002$, exponential decay rates $\rho_1 = 0.5$, $\rho_2 = 0.999$.

%TODO Comparisons of design decisions


%----------------------------------------------------------------------------------------

\subsection{Activation functions}
Another important design decision when building neural networks pertains to the choice of activation function we use in our hidden layers.

In the context of neural network training, an activation function $\varphi$ is one that we apply to the outputs of hidden layers, i.e. $\varphi(y)$ where $y=W^Tx + b$.

What makes activation functions so critical is the fact that they allow us to model non-linear data distributions, enabling us to build more complex representations of our predictions.

Historically, non-linear activations functions like the logistic sigmoid function or tanh function, while successful with certain data distributions, have proven difficult to train, mostly due to their non-zero centered property and slope of the function \citep{DBLP:journals/corr/XuHL16}.
Many activations functions have been introduced in machine learning literature, with some working well with many practical applications.

In this section, we provide an overview of such activation functions, all of which we evaluated in our GAN training experiments.

We trained different Policy GAN models using different combinations of activations functions for the Generator and the Discriminator's hidden layers. The activation we accounted for were the following: sigmoid function, ReLU function (Rectified Linear Unit) \citep{DBLP:journals/corr/AroraBMM16}, Leaky ReLU \citep{DBLP:journals/corr/XuWCL15}, ELU or Exponential Linear Units \citep{DBLP:journals/corr/ClevertUH15}, and SELU or Scaled Exponential Linear Units \citep{DBLP:journals/corr/KlambauerUMH17}.

Following are the definitions of the considered activations functions with their respective gradients, followed, in figure~\ref{fig:activationfns}, by a their visualisation on the x-y axis.\\

\textbf{Sigmoid}:
\begin{flalign}
  \text{sigmoid}(x) = \frac{1}{1+\exp{-x}}
\end{flalign}

\textbf{ReLU}:
\begin{flalign}
  \text{relu}(x) = \max(0, x)
\end{flalign} 

\textbf{Leaky ReLU:}
\begin{flalign}
  \text{lrelu}(x) = 
  \begin{cases} 
      \alpha x      & \quad \text{if } x \leq  0 \\
      x       & \quad \text{if } x > 0 .
    \end{cases} 
\end{flalign} 

\textbf{ELU:}
\begin{equation}
  \text{elu}(x) = 
  \begin{cases} 
      \alpha (\exp(x) - 1)      & \quad \text{if } x \leq  0 \\
      x       & \quad \text{if } x > 0 .
    \end{cases} 
\end{equation} 

\textbf{SELU:}
\begin{equation}
  \text{selu}(x) = \lambda
  \begin{cases} 
      \alpha (\exp(x) - 1)      & \quad \text{if } x \leq  0 \\
      x       & \quad \text{if } x > 0 .
    \end{cases} 
\end{equation} 

\begin{figure}[H]
\vskip -3mm
\begin{center}
\centerline{\includegraphics[width=10cm]{Figures/ActivationFns}}
\caption{Plots of activation functions}
\label{fig:activationfns}
\end{center}
\vskip -3mm
\end{figure}
As we expected, different activations functions yield varying degree of successful results, with Leaky ReLu ($\alpha=0.2$) producing the best overall results for both the Generator and the Discriminator model.

Using all other activations functions in our hidden layer, with the exception of the sigmoid function, yield good results.
The sigmoid activation function model likely incurred incurred into the vanishing gradient problem \citep{DBLP:journals/corr/abs-1211-5063}, where a big amount of possible inputs are 'squashed' into a relatively small range (0, 1). Other activation functions like LeakyReLu, ELU or SELU address this problem by giving a wider range of continuous values for negative inputs.

While the use of the sigmoid function as activation for the hidden layers creates this issue, it is still greatly functional to model a binary classification problem. More specifically, the Discriminator network, which is effectively a binary classifier determining whether the input is 'fake' or 'real', can still make use of a sigmoid activation in its output layer to model the probability of the input being real.

\subsection{Weight initialisation}
A common easy approach to weight initialisation is to use small random values as weights, most commonly sample from a normal distribution $N\sim \mathcal{N}(0, \sigma)$. Having a 'smarter' weight initialisation, however, may lead to  faster convergence when training our models.

One such approach is using sampling from the following uniform distribution:

\begin{equation}
 W \sim U(-\frac{1}{\sqrt{n_{in}}}, \frac{1}{\sqrt{n_{in}}})
\end{equation}

where $n_{in}$ is the number of incoming connections of each unit in the hidden layer.

Another, slightly more sophisticated approach to weight initialisation is the Glorot/Xavier initilisation \citep{pmlr-v9-glorot10a}, whereby weights are sampled from the following uniform distribution:

\begin{equation}
W \sim U\Bigg(-\sqrt{\frac{6}{n_{in}+n_{out}}}, \sqrt{\frac{6}{n_{in}+n_{out}}}\Bigg)
\end{equation}

where, once again, $n_{in}$ is the number of incoming connections of each neuron in the hidden layer. Similarly, $n_{out}$ is the number of outgoing connections.

We experimented will all these three variations to initialise the weights in our neural network. Since there is randomness involved in these techniques, we ran the GAN training multiple times. On average, Glorot/Xavier initialisation led to 6\% fewer epochs required before both the generator and the discriminator converged.

\subsection{Batch Normalisation}
Batch Normalisation \citep{DBLP:journals/corr/IoffeS15} is a technique that improves stability and performance of neural networks by normalising the inputs of each layer such that they have a mean output activation of zero and standard deviation of one. Benefits of using batch normalisation include faster training time (i.e. faster convergence to optimal model). This also allows higher learning rates and and makes weight initialisation not as critical.

We first compute the mean and variance of each hidden unit activation across the minibatch (size $M$):
\begin{gather*}
\mu_i \leftarrow \frac{1}{M} \sum_{m=1}^{M}u_i^m \\
\sigma_i^2 \leftarrow \frac{1}{M} \sum_{m=1}^{M}(u_i^m - \mu_i)^2
\end{gather*}

The result of doing batch normalisation will then be:
\begin{gather*}
u_i = w_ix \\
\hat{u_i} = \frac{u_i-\mu_i}{\sqrt{\sigma_i^2 + \epsilon}} \\
z_i = \gamma_i\hat{u_i}+\beta_i = \text{batchNorm}({u_i})
\end{gather*}

where $\gamma$ and $\beta$ are parameters updated with gradient descent that will scale and shift the normalised activations.

We trained two different models for both the Generator and the Discriminator models: one with Batch Normalisation and one without, keeping the rest of the architecture and hyperparameter settings fixed.
Using Batch Normalisation in the Generator network leads to much faster convergence than the model that does not make use of it.
The Discriminator network, effectively being a much less complex model than the Generator, did not benefit from batch-normalised inputs in terms of speed of model convergence and overall cost.