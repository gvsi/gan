% Chapter 2

\chapter{Background} % Main chapter title

\label{Chapter2} % For referencing the chapter elsewhere, use \ref{Chapter2} 


%----------------------------------------------------------------------------------------

\section{Reinforcement Learning}
\label{RL}

\subsection{Markov Decision Processes}
\label{MDP}
Environments in traditional reinforcement learning application are usually modelled as Markov Decision Processes or MDPs.

These can be formulated as systems with the following components:
\begin{itemize}
	\item Finite set of states $S=\{s_0,\ldots,s_n\}$ and actions $A=\{a_0,\ldots,a_m\}$.
	\item A distribution of probabilities $P_a(s,s')$ for transitions from state $s$ to $s'$ for each possible action $a$.
	\item A reward function $R:S\mapsto\mathbb{R}$ for being at a particular state.
	\item The goal in reinforcement learning is to maximise the final reward that an agent achieves in the environment.
\end{itemize}

As the name suggests, MDPs obey the \textit{Markov property}, whereby the probability of the system being in a certain future state exclusively depends upon the present state, and not upon an arbitrarily-long sequence of past states.

In figure~\ref{fig:MDP} we show a sample schematic of a Markov Decision Process, and how states, actions, and rewards could be connected between each other.

\begin{figure}
\centering
\includegraphics[width=10cm]{Figures/MDP}
\caption{Sample schematic of an MDP.}
\label{fig:MDP}
\end{figure}


\subsection{Q-learning}
\label{qlearning}
Now that we contextualised reinforcement learning environments as Markov Decision Processes, we can introduce the final objective of reinforcement learning tasks: finding a function $\Pi:S\mapsto A$ called \textbf{policy}, that maps the appropriate action $a\in A$ given the current state $s\in S$, as to maximise our agent's final reward.

In particular, we will now present Q-learning \parencite{watkins1992q}, an algorithm that yields an optimal policy given an MDP.

Q-learning lets us learn the \emph{quality}, or expected utility, for each state-action combination. That is, for each state, let's estimate all the expected rewards we obtain by taking each possible action at that particular state.

More formally, we estimate a function $Q: S \times A \to \mathbb{R}$. We can model $Q$ as a mapping table (initialised with some uniform values), whose value we update at each time step of our simulations.

Here's how we update our Q-table at each time step $t$:
  \[Q(s_{t},a_{t}) = \underbrace{Q(s_t,a_t)}_{\rm old~value} +
  \underbrace{\alpha}_{\rm learning~rate} \times \left[
    \overbrace{\underbrace{r_{t+1}}_{\rm reward} + \underbrace{\gamma}_{\rm
        discount~factor} \underbrace{\max_{a}Q(s_{t+1}, a)}_{\rm
        estimate~of~optimal~future~value}}^{\rm learned~value} -
    \underbrace{Q(s_t,a_t)}_{\rm old~value} \right] \]

% TODO: Clean this up    
\[Q_{t+1} = Q_{t}-\underbrace{\beta}_{\rm discriminator~learning~rate} \times \underbrace{\nabla D(Q_{t})}_{\rm gradient~of~D~at~Q_{t}}\]

  \[Q(s_{t},a_{t}) = \underbrace{Q(s_t,a_t)}_{\rm old~value} +
  \underbrace{\alpha}_{\rm learning~rate} \times \left[
    \overbrace{\underbrace{r_{t+1}}_{\rm reward} + \underbrace{\gamma}_{\rm
        discount~factor} \underbrace{\max_{a}Q(s_{t+1}, a)}_{\rm
        estimate~of~optimal~future~value}}^{\rm learned~value} -
    \underbrace{Q(s_t,a_t)}_{\rm old~value} \right]\]
    
    \[- \underbrace{\beta}_{\rm discriminator~learning~rate} \times \underbrace{\nabla D(Q)(s_{t}, a_{t})}_{\rm gradient~of~D~at~Q_{t}} \]
   
   \[\frac{\sum_{i}^{test~set} valid\_score(Q_i) - valid\_score(Q_i')}{\#(test~set)}\]
where:
\begin{itemize}
	\item $\alpha\in[0,1]$ is the \emph{learning rate}, a coefficient that regulates how much the newly learned values will contribute in the update
	\item $\gamma\in[0,1]$ is the \emph{discount factor}, a coefficient that controls the weight of future rewards. Values closer to 0 will make our agent "short-sighted", considering only the immediate rewards.
\end{itemize}

What is our optimal policy when we do Q-learning then? After training, it is simply that function $\pi: S \to A$ that, for each state, returns the action with maximum expected utility in our Q-table.

\subsection{Exploration/Exploitation tradeoff}
An important theme in reinforcement learning, and that we heavily focus our attention on in our project is the idea of the tradeoff between Exploration and Exploitation.

\subsection{Deep Reinforcement Learning}
\subsubsection{DQN}
\subsection{Problems with reinforcement learning techniques}
\subsection{Actor critic}

%----------------------------------------------------------------------------------------

\section{Generative Adversarial Networks}
\label{sec:gan}
Generative Adversarial Networks, or GANs, are a deep neural network architecture composed of two neural networks, set against each other (thus "adversarial").

GANs were introduced in a paper by \cite{goodfellow2014generative}. Yann LeCun, Facebook's AI Research director referred GANs as being "the most interesting idea in the last 10 years in ML".

What have made GANs particularly successful in past work is their ability to model any distribution of data, and provide, after training, both a generative model and a discriminative model based on the initial distribution of the data. Let us define what we mean by these terms.

A \textit{generative} model is one that is able to produce new data points that fit the distribution of the training data. For example, a Gaussian Mixture Model is a model that, after training, could generate new data based on the original distribution. More formally, training a generative model can be seen as maximum likelihood estimation problem:

\[\mathscr{L} = \prod_{i=1}^{m} p_{model}(x_i;\theta)\]

More specifically, we want to maximise the likelihood that a sample $x$ that the generative model produces belongs to the distribution of our initial data.

A \textit{discriminative} model is one that can discern (or "discriminate") the difference between two (or more) classes/labels. For example, we could train a Convolutional Neural Network that is able to tell us whether an input image is a face (1) or not (0).

Depending on the task at hand, we may be more interested in the generative model (for example in the case of image synthesis), or in the discriminative model, or in both.

Next, we will look at the typical architecture of Generative Adversarial Networks.

\subsection{Architecture of GANs}
Since GANs were introduced in 2014, many variants of GANs spawned from the research community. Here we will focus on the architecture of the original, vanilla GAN.

All variations of GANs will nevertheless contain two main components, which are both modelled as neural networks. These two models are trained simultaneously. One is the \emph{generator} model, that takes an input \emph{z} and produces a sample \emph{x} from an implicit probability distribution. The other is the \emph{discriminator}, a classifier that, given a sample, tries to identify it as originating from the generator model or from the original distribution.

An intuition to this set up is the following: the generator \code{G} can be imagined as a counterfeiter trying to produce fake banknotes or paper money; the discriminator \code{D} is the police, trying to distinguish real banknotes from the fake ones generated by the counterfeiter. As the counterfeiter becomes better and better at producing banknotes, the police will also try to improve its counterfeiting techniques.

More formally, \code{G} is a differentiable function whose parameters are trained to minimise correct assignments of \code{D}. \code{D} is also a differentiable function, which has been trained to maximise correct labels to the real and the fake samples.

In figure~\ref{fig:VanillaGAN} summarises this whole procedure by showing the architecture of GANs.

At each iteration, the mini-batch inputs to the discriminator \code{D} are taken from both the real data sampled from the training data, and the samples generated by the generator, whose inputs is random latent variable $z$. Given \code{D}'s prediction, we then apply a gradient-based optimisation method to update both \code{D}'s and \code{G}'s parameter.

To do so, we follow the loss function that defined based on the goals of each network. This cost function implicitly models a zero-sum game, which in game theory are guaranteed to achieve an equilibrium, by the minimax theorem \citep{du2013minimax}.

The cost function for the whole system would look like the following minimax game:
\[\min_{G} \max_{D} V(G, D) = \mathbb{E}_{x\sim p_{data}(x)}[log D(x)] + \mathbb{E}_{z \sim p_{g}(z)}[log(1-D(G(z)))] \]

\begin{figure}
\centering
\includegraphics[width=15cm]{Figures/VanillaGAN}
\caption{Architecture of a Generative Adversarial Network}
\label{fig:VanillaGAN}
\end{figure}

\subsection{Successes}
\label{successes}
\subsubsection{DCGAN}
\subsection{Conditional GANs}

%----------------------------------------------------------------------------------------

\section{Existing related work}
\subsection{Generative Adversarial Imitation Learning}

%----------------------------------------------------------------------------------------