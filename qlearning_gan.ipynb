{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from json_tricks import dumps, loads\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pickle\n",
    "import socket\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "import gym\n",
    "from gym.envs.registration import register, spec\n",
    "from gym import envs\n",
    "\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model, save_model\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "lines = []\n",
    "# with open(\"data/data4_valid/fl4-comb-new.txt\") as f:\n",
    "with open(\"data/data4_valid_mul5/fl4-mul5-comb-new.txt\") as f:\n",
    "    for line in f:\n",
    "        lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3828/3828 [00:20<00:00, 186.56it/s]\n"
     ]
    }
   ],
   "source": [
    "Qs = []\n",
    "str_combs = []\n",
    "alls = {}\n",
    "for i in tqdm(range(len(lines))):\n",
    "    map_str_comb, dic = lines[i].split(\"\\t\")\n",
    "    obj = loads(dic)\n",
    "    alls[map_str_comb] = obj['Q']\n",
    "    Qs.append(obj['Q'])\n",
    "    str_combs.append(map_str_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_data = np.array([q for q in Qs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(q_data, test_size=0.10, random_state=42)\n",
    "X_train_str, X_test_str = train_test_split(str_combs, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gvsi/anaconda3/envs/gan/lib/python3.6/site-packages/keras/models.py:291: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "  warnings.warn('Error in loading the saved optimizer '\n"
     ]
    }
   ],
   "source": [
    "# G_params = pickle.load(open(\"data/gan/G_params.p\", \"rb\"))\n",
    "# D_params = pickle.load(open(\"data/gan/D_params.p\", \"rb\"))\n",
    "\n",
    "discriminator = load_model(\"data/gan/D_params_keras.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gvsi/anaconda3/envs/gan/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16, 4, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(X_test[0], axis=3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test2 = np.expand_dims(X_test, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wzh, bzh, Whx, bhx = G_params\n",
    "Wxh, bxh, Why, bhy = D_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_size = 64\n",
    "ones_label = Variable(torch.ones(mb_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def D(X):\n",
    "    h = nn.relu(X @ Wxh + bxh.repeat(X.size(0), 1))\n",
    "    y = nn.sigmoid(h @ Why + bhy.repeat(h.size(0), 1))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discriminator_score(Q):\n",
    "    if len(Q.shape) != 1:\n",
    "        Q = Q.flatten()\n",
    "    inputs = Variable(torch.from_numpy(Q).float(), requires_grad=True)\n",
    "    myD = D(inputs)\n",
    "    return myD.data.numpy()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(Q):\n",
    "    if len(Q.shape) != 1:\n",
    "        Q = Q.flatten()\n",
    "    inputs = Variable(torch.from_numpy(Q).float(), requires_grad=True)\n",
    "    myD = D(inputs)\n",
    "    myD.backward(ones_label)\n",
    "    return inputs.grad.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.33704624e+01,  2.23418465e+01, -2.79528713e+00,  3.64787388e+00,\n",
       "       -3.98031235e-01,  3.28875999e+01, -4.82095337e+01, -2.14143658e+01,\n",
       "       -9.34957581e+02, -1.00309332e+03, -6.53250793e+02, -1.49343124e+02,\n",
       "       -5.58164673e+01, -6.09358139e+01, -1.06908310e+02, -8.02173615e-02,\n",
       "       -2.56729050e+01, -1.74180365e+00, -5.37576103e+01, -3.72171822e+01,\n",
       "       -1.54025284e+02,  3.98059654e+00,  3.78225174e+01, -1.50335560e+01,\n",
       "       -8.32711670e+02, -7.80168335e+02, -2.62103455e+02, -8.77553589e+02,\n",
       "       -5.18191681e+01, -2.07699852e+01,  5.84339619e-01, -7.88252335e+01,\n",
       "       -1.22757645e+02, -3.13745911e+02, -3.43434906e+02, -5.72222672e+01,\n",
       "       -6.97571655e+02, -9.58487793e+02, -3.73693268e+02, -4.26875275e+02,\n",
       "       -4.57048950e+02, -7.45678425e+00, -5.32991333e+02,  2.39936113e+00,\n",
       "       -1.94286194e+02, -2.81333984e+02, -2.43350159e+02, -1.94121796e+02,\n",
       "        5.09861279e+00,  3.33634949e+01,  5.99459648e+00,  2.07424393e+01,\n",
       "       -3.10667358e+02, -2.33745575e+02, -8.23421753e+02, -7.21736084e+02,\n",
       "       -3.22065063e+02, -2.90578217e+02, -3.97936249e+02, -3.18251434e+02,\n",
       "       -2.04225037e+02, -1.67360748e+02, -1.82486191e+02, -1.75255798e+02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_gradients(np.zeros(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SHHFFFFFHFHFFFHG'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_str[68]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = discriminator.outputs[-1]\n",
    "loss = (1 - output) ** 2\n",
    "grad = K.gradients(loss, discriminator.inputs[-1])\n",
    "input_tensors = [discriminator.inputs[0]]\n",
    "get_gradient = K.function(inputs=input_tensors, outputs=grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_env(env_map, slippery=True, MY_ENV_NAME='FrozenLakeNonskid-v0'):\n",
    "    if MY_ENV_NAME in envs.registry.env_specs:\n",
    "        envs.registry.env_specs.pop(MY_ENV_NAME)\n",
    "\n",
    "    register(\n",
    "        id=MY_ENV_NAME,\n",
    "        entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "        kwargs={'is_slippery': slippery, 'desc': env_map},\n",
    "        max_episode_steps=100,\n",
    "        reward_threshold=0.78, # optimum = .8196\n",
    "    )\n",
    "    env = gym.make(MY_ENV_NAME)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify(env, Q, num_episodes = 10000):\n",
    "    print(\"Running validation...\")\n",
    "    # Set learning parameters\n",
    "    #create lists to contain total rewards and steps per episode\n",
    "    #jList = []\n",
    "    rList = []\n",
    "    successes = 0\n",
    "    jTot = 0\n",
    "    for i in tqdm(range(num_episodes)):\n",
    "        #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        #The Q-Table learning algorithm\n",
    "        while j < 200:\n",
    "            j+=1\n",
    "            #Choose an action by greedily (with noise) picking from Q table\n",
    "            a = np.argmax(Q[s,:])\n",
    "            #Get new state and reward from environment\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            if d == True and r > 0:\n",
    "                jTot += j\n",
    "                successes += 1\n",
    "            if d == True:\n",
    "                break\n",
    "        rList.append(rAll)\n",
    "    print(\"Score over time: \" +  str(sum(rList)/num_episodes))\n",
    "    valid_score = sum(rList)/num_episodes\n",
    "    try:\n",
    "        avg_steps = jTot / successes\n",
    "    except:\n",
    "        avg_steps = 0\n",
    "    return valid_score, avg_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment(object):\n",
    "    def __init__(self, env, num_episodes=10000):\n",
    "        self.env = env\n",
    "        self.Q = np.zeros([self.env.observation_space.n, self.env.action_space.n])\n",
    "        self.machine = socket.gethostname()\n",
    "        self.num_episodes = num_episodes\n",
    "        self.done = False\n",
    "        self.score = None\n",
    "        self.start = None\n",
    "        self.end = None\n",
    "        self.train_successes = []\n",
    "        self.valid_score = None\n",
    "        self.valid_avg_steps = None\n",
    "\n",
    "    def print_score(self):\n",
    "        if not self.done:\n",
    "            print(\"Run first.\")\n",
    "            return\n",
    "        print(\"Score over time: \" +  str(self.score))\n",
    "\n",
    "    def run(self):\n",
    "        print(\"Running experiment...\")\n",
    "        if self.done:\n",
    "            print(\"Already done running\")\n",
    "            return\n",
    "\n",
    "        self.start = datetime.now()\n",
    "        lr = .8\n",
    "        lr_gan = 0.001\n",
    "        e = 0.1\n",
    "        y = .95\n",
    "        #create lists to contain total rewards and steps per episode\n",
    "        jList = []\n",
    "        rList = []\n",
    "        su1 = 0\n",
    "        su_tot = 0\n",
    "        \n",
    "        for i in tqdm(range(self.num_episodes)):\n",
    "            #Reset environment and get first new observation\n",
    "            s = self.env.reset()\n",
    "            rAll = 0\n",
    "            d = False\n",
    "            j = 0\n",
    "            #The Q-Table learning algorithm\n",
    "            while j < 200:\n",
    "                j+=1\n",
    "                #Choose an action by greedily (with noise) picking from Q table\n",
    "                a = None\n",
    "                if random.uniform(0,1) < e:\n",
    "                    a = self.env.action_space.sample()\n",
    "                else:\n",
    "                    a = np.argmax(self.Q[s,:])\n",
    "                #Get new state and reward from environment\n",
    "                s1,r,d,_ = self.env.step(a)\n",
    "                if d == True and r != 1:\n",
    "                    self.Q[s, a] -= 0.01\n",
    "                self.Q[s,a] = self.Q[s,a] + lr*(r + y*np.max(self.Q[s1,:]) - self.Q[s,a])\n",
    "#                 print(discriminator.predict(np.expand_dims(np.array([self.Q]), axis=3)))\n",
    "#                 print(np.expand_dims(self.Q, axis=3))\n",
    "                #Update Q-Table with new knowledge\n",
    "                if i < 500:\n",
    "                    self.Q[s,a] = self.Q[s,a] + lr*(r + y*np.max(self.Q[s1,:]) - self.Q[s,a])\n",
    "                    self.Q[s,a] = self.Q[s,a] - 0.5 * np.reshape(get_gradient([np.array([np.expand_dims(self.Q, axis=3)])]), (16, 4))[s,a]\n",
    "                else:\n",
    "#                     if np.sign((r + y*np.max(self.Q[s1,:]) - self.Q[s,a])) == np.sign(0.5 * np.reshape(get_gradient([np.array([np.expand_dims(self.Q, axis=3)])]), (16, 4))[s,a]):\n",
    "#                         su1 += 1\n",
    "#                     su_tot += 1\n",
    "#                     old_Q = self.Q\n",
    "#                     old_Q[s,a] = self.Q[s,a] + lr*(r + y*np.max(self.Q[s1,:]) - self.Q[s,a])\n",
    "#                     self.Q[s,a] = self.Q[s,a] + get_discriminator_score(old_Q)*(r + y*np.max(self.Q[s1,:]) - self.Q[s,a])\n",
    "#                     self.Q[s,a] = self.Q[s,a] + lr*(r + y*np.max(self.Q[s1,:]) - self.Q[s,a])\n",
    "#                     self.Q = self.Q + lr_gan * compute_gradients(self.Q).reshape((self.env.observation_space.n, self.env.action_space.n))\n",
    "                    self.Q[s,a] = self.Q[s,a] + lr*(r + y*np.max(self.Q[s1,:]) - self.Q[s,a])\n",
    "#                     print(get_gradient([np.array([np.expand_dims(self.Q, axis=3)])]))\n",
    "#                     self.Q[s,a] = self.Q[s,a] - 0.5 * np.reshape(get_gradient([np.array([np.expand_dims(self.Q, axis=3)])]), (16, 4))[s,a]\n",
    "#                     print((r + y*np.max(self.Q[s1,:]) - self.Q[s,a]), np.reshape(get_gradient([np.array([np.expand_dims(self.Q, axis=3)])]), (16, 4))[s,a])\n",
    "#                     print(np.sign((r + y*np.max(self.Q[s1,:]) - self.Q[s,a])), np.sign(0.5 * np.reshape(get_gradient([np.array([np.expand_dims(self.Q, axis=3)])]), (16, 4))[s,a]))\n",
    "    \n",
    "                rAll += r\n",
    "                s = s1\n",
    "                if d == True and r > 0:\n",
    "                    self.train_successes.append((i, j))\n",
    "                if d == True:\n",
    "                    #Reduce chance of random action as we train the model.\n",
    "        #             e = 1./((i/50) + 10)\n",
    "                    break\n",
    "            rList.append(rAll)\n",
    "#         print(su1/su_tot)\n",
    "        self.done = True\n",
    "        self.end = datetime.now()\n",
    "        self.score = sum(rList)/self.num_episodes\n",
    "\n",
    "    def validate(self):\n",
    "        valid_score, avg_steps = verify(self.env, self.Q)\n",
    "        self.valid_score = valid_score\n",
    "        self.valid_avg_steps = avg_steps\n",
    "\n",
    "    def dumps(self):\n",
    "        if not self.done:\n",
    "            print(\"Run first.\")\n",
    "            return\n",
    "\n",
    "        return dumps({'Q': self.Q,\n",
    "                      'start': self.start,\n",
    "                      'end': self.end,\n",
    "                      'train_score': self.score,\n",
    "                      'num_episodes': self.num_episodes,\n",
    "                      'train_successes': exp.train_successes,\n",
    "                      'train_machine' : self.machine,\n",
    "                      'valid_score' : self.valid_score,\n",
    "                      'valid_avg_steps' : self.valid_avg_steps\n",
    "                      })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = X_train_str[67]\n",
    "n = 4\n",
    "map_str_split = [line[i:i+n] for i in range(0, len(line), n)]\n",
    "env = new_env(map_str_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3000 [00:00<?, ?it/s]/Users/gvsi/anaconda3/envs/gan/lib/python3.6/site-packages/ipykernel_launcher.py:63: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n",
      "  1%|          | 21/3000 [00:00<00:15, 189.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHHF\n",
      "HFFF\n",
      "HFHG\n",
      "Running experiment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:03<00:00, 862.15it/s]\n"
     ]
    }
   ],
   "source": [
    "env.render()\n",
    "exp = Experiment(env, num_episodes=3000)\n",
    "exp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.01665078e-01,  3.19791822e-01,  1.04977438e-01,\n",
       "         2.96964823e-01],\n",
       "       [ 1.63992919e-02,  2.13699419e-02,  1.76355078e-02,\n",
       "         3.75075462e-01],\n",
       "       [ 5.73409099e-01,  1.84459168e-02,  2.68229480e-02,\n",
       "         5.44371782e-01],\n",
       "       [ 4.10917253e-01,  7.56062812e-01,  3.08908570e-01,\n",
       "         3.17230141e-01],\n",
       "       [-4.16665796e-04, -4.16610657e-04, -4.16165312e-04,\n",
       "         1.29921919e-02],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [ 8.10509498e-01,  9.35591386e-01,  7.08317648e-01,\n",
       "         8.31503131e-01],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [-4.16000000e-04, -4.00000000e-04, -3.08005102e-04,\n",
       "        -4.00000000e-04],\n",
       "       [-4.16665600e-04, -4.16453895e-04,  8.68860142e-01,\n",
       "        -4.16660560e-04],\n",
       "       [ 7.28784241e-02,  8.83438322e-01,  9.96484344e-01,\n",
       "         6.19622077e-01],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [-4.00000000e-04, -4.00000000e-04, -3.78449635e-04,\n",
       "        -4.00000000e-04],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00]])"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13833333333333334"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 658/10000 [00:00<00:01, 6567.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:01<00:00, 7517.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "exp.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gan)",
   "language": "python",
   "name": "gan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
